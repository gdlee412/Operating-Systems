# Advanced Scheduling Schemes

## Proportional Share Scheduler

-   Fair-share scheduler
    -   **`Proportional share scheduler`**
    -   Guarantee that each job obtain _a certain percentage_ of CPU time
    -   Not optimized for turnaround or response time

## Lottery scheduling

-   Tickets

    -   Represent the share of a resource that a process should receive
    -   <u>The percent of tickets</u> represents its share of the system resource in question

-   Example

    -   There are two process, A and B
        -   Process A has 75 tickets -> receive 75% of the CPU
        -   Process B has 25 tickets -> receive 25% of the CPU

-   The scheduler picks <u>a winning ticket</u>

    -   Load the state of that _winning process_ and runs it

-   Example
    -   There are 100 tickets
        -   Process A has 75 tickets: 0 ~ 74
        -   Process B has 25 tickets: 75 ~ 99

```
Scheduler's winning tickets: 63 85 70 39 76 17 29 41 36....

Resulting scheduler: A B A A B A A A A....
```

`The longer these two jobs compete, The more likely they are to achieve the desired percentages.`

### Ticket Mechanisms

-   Ticket currency

    -   A user allocates tickets among their own jobs in whatever currency they would like
    -   The system converts the currency into the correct global value
    -   Example
        -   There are 200 tickets (Global currency)
        -   Process A has 100 tickets
        -   Process B has 100 tickets

-   The User creates a local currency to locally split the CPU
    -   OS doesn't care about what happens
    -   To locally give priority to a certain process

```
User A -> 500 (A's currency) to A1 -> 50 (global currency)
User A -> 500 (A's currency) to A2 -> 50 (global currency)

User B -> 10 (B's currency) to B1 -> 100 (global currency)
```

    - User can be a child or the user, etc.

-   Ticket transfer

    -   A process can temporarily <u>hand off</u> _its tickets_ to another process

    -   When a process(with high priority) needs a resource, but another process(low priority) is using it, they can temporarily give tickets to another process
        -   increases the other process's chance of getting the CPU resource

-   `Priority Inversion`

    -   Process ABC exists
    -   A needs printer but C is using it
    -   Since priority of B is higher than C, B steals C's CPU
    -   Priority gets inverted
    -   So A gives tickets to C for faster process, to allow C to finish using the printer
    -   Mars pathfinder
        -   Discovered by Lui Sha

-   Ticket inflation
    -   A process can <u>temporarily raise or lower</u> the number of tickets it owns
    -   If any one process needs _more CPU time_, it can boost its tickets
    -   the other processes must _allow_ for this to happen
        -   Child processes
        -   Same user applications
        -   etc.

### Implementation

-   Example: there are three processes, A, B, and C.
    -   Keep the processes in a list

head -> Job A: Tix 100 -> Job B: Tix 50 -> Job C: Tix 250 -> NULL

```c
// counter: used to track if we’ve found the winner yet
int counter = 0;

// winner: use some call to a random number generator to
// get a value, between 0 and the total # of tickets
int winner = getrandom(0, totaltickets);

// current: use this to walk through the list of jobs
node_t *current = head;

// loop until the sum of ticket values is > the winner
while (current) {
    counter = counter + current->tickets;
    if (counter > winner)
        break; // found the winner
    current = current->next;
    }
    // ’current’ is the winner: schedule it...
```

-   U: unfairness metric

    -   The time the first job completes divided by the time that the second job completes

-   Example
    -   There are two jobs, each job has runtime 10
        -   First job finishes at time 10
        -   Second job finishes at time 20
    -   $ U = \frac{10}{20} = 0.5$
    -   U will be close to 1 when both jobs finish at nearly the same time

### Lottery Fairness Study

-   There are two jobs - Each jobs has the same number of tickers (100)
    > When the job length is not very long, average unfairness can be **quite severe**

## Stride Scheduling

-   Stride of each process

    -   (A large number) / (the number of tickets in the process)
    -   Example: A large number = 10,000
        -   Process A has 100 tickets -> stride of A is 100
        -   Process B has 50 tickers -> stride of B is 200

-   A more important process should have a smaller stride

    -   the OS picks the smaller stride
    -   since the important process has a smaller stride, the OS picks the important process more often

-   A process runs, increment a counter(=pass value) for it by its stride
    -   Pick the process to run that has the lowest pass value

```c
//pseudo code implementation
current = remove_min(queue); // pick client with minimum pass
schedule(current); // use resource for quantum
current->pass += current->stride; // compute next pass using stride
insert(queue, current); // put back into the queue
```

### Example

| Pass A(stride = 100) | Pass B (stride = 200) | Pass C(stride = 40) | Who Runs? |
| -------------------- | --------------------- | ------------------- | --------- |
| 0                    | 0                     | 0                   | A         |
| 100                  | 0                     | 0                   | B         |
| 100                  | 200                   | 0                   | C         |
| 100                  | 200                   | 40                  | C         |
| 100                  | 200                   | 80                  | C         |
| 100                  | 200                   | 120                 | A         |
| 200                  | 200                   | 120                 | C         |
| 200                  | 200                   | 160                 | C         |
| 200                  | 200                   | 200                 | ...       |

> If new job enters with pass value 0, It will **monopolize** the CPU

- CFS
    - similar to stride scheduling
    - choose the lowest virtual runtime

## Multiprocessor Scheduling
- The rise of the `multicore processor` is the source of multiprocessor-scheduling proliferation
    - **Multicore**: multicore CPU cores are packed onto a single chip
- Adding more CPUs <u>does not</u> make that single application runfaster -> You'll have to rewrite application to run in parallel, using **threads**

>How to schedule jobs on **Multiple CPUs**?

### Single CPU with Cache
- Cache
    - Small, fast memories
    - Hold copies of <u>popular</u> data that is found in the main memory
    - Utilize *temporal* and *spatial* locality

- Main Memory
    - Holds all of the data
    - Access to main memory is slower than cache

> By keeping data in cache, the system can make slow memory appear to be a fast one

### Cache Coherence
- Consistency of shared resource data stored in multiple caches

0. Two CPUs with caches sharing memory
1. CPU0 reads a data at address 1.
2. data *D* is updated and CPU1 is scheduled (Only CPU0's cache data is updated)
3. CPU1 re-reads the value at address A
    > CPU1 gets the ``old value D`` instead of the correct value D'

#### Cache Coherence Solution
- Bus snooping
    - Each cache pays attention to memory updates by **observing the bus**
    - When a CPU sees an updae for a data item it holds in its cache, it will notice the change and either <u>invalidate</u> its copy or <u>update</u> it

### Don't forget Synchronization
- When accessing shared data across CPUs, `mutual exclusion` primitives should likely be used to <u>guarantee correctness</u>